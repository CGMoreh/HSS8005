[
  {
    "objectID": "plan8.html",
    "href": "plan8.html",
    "title": "HSS8005 {{< iconify line-md plus >}}",
    "section": "",
    "text": "Topic plan and materials\n\n\n\n\n\n\nWeek\n\n\nTW\n\n\nDate\n\n\nTopic\n\n\nInfo\n\n\nNotes\n\n\nLecture\n\n\nLabs\n\n\nHandouts\n\n\n\n\n\n\n\n\n22\n\n\n30 January\n\n\nIntroduction: module overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n22\n\n\n02 February\n\n\nGamblers, God, Guinness and peas  A brief history of statistics\n\n\n\n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n23\n\n\n09 February\n\n\nRevisiting Flatland  A review of general linear models\n\n\n\n\n\n  \n\n\n  \n\n\n\n\n\n  \n\n\n\n\nWeek 3\n\n\n24\n\n\n16 February\n\n\nDear Prudence, Help! I may be cheating with my X  Interactions and the logic of causal inference\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\nWeek 4\n\n\n25\n\n\n23 February\n\n\nThe Y question  Generalised linear models\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\nWeek 5\n\n\n26\n\n\n02 March\n\n\nDo we live in a simulation?  Basic data simulation for statistical inference and power analysis\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\nWeek 6\n\n\n27\n\n\n09 March\n\n\nChallenging hierarchies  Multilevel models\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\nWeek 7\n\n\n28\n\n\n16 March\n\n\nThe unobserved  Latent variables and structural models\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\nWeek 8\n\n\n29\n\n\n23 March\n\n\nWords, words, mere words…  Text as data\n\n\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n\n\n\n\n29\n\n\n24 March\n\n\nConclusions: sum up, divide, and conquer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic details\n\n\n\n\n\nTopic\n\n\nDescription\n\n\n\n\n\n\nWeek 1  Gamblers, God, Guinness and peas  A brief history of statistics\n\n\nIn the first contribution to a series of articles on the history of probability and statistics in the journal Biometrika, Florence Nightingale David (1955) (no linear relationship with the famous social reformer) paraphrased a contemporary archaeologist who quipped that “a symptom of decadence in a civilization is when men become interested in their own history”, giving the interest in his own discipline as proof of the validity of his statement. David, however, thought that this does not stand true also for scientists’ and statisticians’ own emerging interest in their disciplines. He was right, in that the critical examination of the intellectual development of statistics and probability theory that followed has improved the discipline by excavating ideas that had been buried by mainstream statistics, but he was also mistaken, in that this activity threw light on the decadence of mainstream statistical practice. In this lecture we will look back on the development of some basic statistical concepts and learn about the ideas and preoccupations that influenced them over the centuries. The aim of this overview is to build up essential intuition about the concepts and methods that we will learn later. Brains-on activities will include casting astragali, fighting Laplace’s Demon, tasting tea, and comparing peas in a pod. By the end, we will gain a clearer understanding of the limits of statistical analysis and the dangers of not acknowledging those limits.\nThe IT lab will provide a very hands-on practical introduction to the statistical software that will be used in the module.\n\n\n\n\nWeek 2  Revisiting Flatland  A review of general linear models\n\n\nIn Edwin Abbott’s 1884 novella, the inhabitants of Flatland are geometric shapes living in a two-dimensional world, incapable of imagining the existence of higher dimensions. A sphere passing through the plain of their world is a fascinating but incomprehensible event: Flatlanders can only see a dot becoming a circle, increasing in circumference, then shrinking back in size and disappearing. There are, in this universe, worlds with even more limited views, like the one-dimensional Lineland and the zero-dimensional Pointland. Any attempt to expand the perspective of their inhabitant(s) is doomed to failure. But as in any good adventure story, a chosen Flatland native embarks on a journey of discovery and revelation - and ostracism and imprisonment. The story is interpreted as an allegorical criticism of Victorian-age social structure, but can equally describe the limitations of inhabiting uncritically a methodological world in which all data are ‘normal’ and all relationships are linear. Moving beyond linearity and acquiring the statistical intuition needed to think in higher dimensions and perceive more complex relationships is indeed a matter of practice-induced revelation. It’s unlikely that we will reach statistical nirvana in this short course, but we’ll attempt to build some more substantial structures upon the arid plains of linear regression. We start by looking around in the Flat-, Line- and Point-lands of quantitative analysis. Incorrigible procrastinators may want to check out a full-length computer animated film version of Flatland on YouTube. Others may be better served by this brief TED-Ed animation.\n\n\n\n\nWeek 3  Dear Prudence, Help! I may be cheating with my X  Interactions and the logic of causal inference\n\n\nMuch of what we do in quantitative data analysis is about examining relationships. We are often interested in proposing and testing models of relationships between two or more variables. Sometimes our variables cry out to us begging for help, and we turn into agony aunts and uncles to our data. Other times we must psychoanalyse our data to uncover hidden associations and interactions. This is not an easy task. Do it carelessly, and you may unwittingly cheat yourself and the readers of your research. This week we’ll build some intuition for detecting complex and uneasy relationships within the design matrix X - that promiscuous commune on the right-hand-side of our regression equations. We’ll expand on the linear additive models that we looked at in the previous week by considering interactions among our predictor variables, we’ll explore the possibilities and challenges of asking causal questions of observational data, and we’ll think about ways to avoid what evolutionary anthropologist Richard McElreath calls ‘causal salad’. We may get an uncomfortable feeling that we may have cheated with our Xs in the past, but we’ll look towards the future. By the way, Dear Prudence is Slate magazine’s advice column; I like the name because being prudent really is essential in data analysis and interpretation. If you’re done with the readings for this week, you may indulge in some Prudie advice on matters more serious than statistics.\n\n\n\n\nWeek 4  The Y question  Generalised linear models\n\n\nIt wasn’t until the last quarter of the 20th century that a unified vision of statistical modelling emerged, allowing practitioners to see how the general linear model we have explored so far is only a specific case of a more general class of models. We could have had a fancy, memorable name for this class of models - as John Nelder, one of its inventors, acknowledged later in life (Senn 2003:127) - but back then academics were not required to undertake marketing training on the tweetabilty-factor of the chosen names for their theories; so we ended up with “generalised linear models”. These models can be applied to explananda (“explained”, “response”, “outcome”, “dependent” etc. variables, our ys) whose possible values have certain constraints (such as being limited by a lower bound or constrained to discreet choices) that makes the parameters of the Gaussian (‘normal’) distribution inefficient in describing them. Instead, they follow some of the other “exponential distributions” (and not only the exponential: cf. Gelman et al. (2020:264)), of which the Poisson, gamma, beta, binomial and multinomial are probably the most common in human and social sciences research. Their “generalised linear modelling” involves mapping them unto a linear model using a so-called “link function”. We will explore what all of this means in practice and how it can be applied to data that we are interested in most in our respective fields of study.\n\n\n\n\nWeek 5  Do we live in a simulation?  Basic data simulation for statistical inference and power analysis\n\n\nWe have known ever since science-fiction author Philip K. Dick’s memorable “Metz address” of 1977 that our world is a computer simulation. Of course, like some common-currency theories in the social sciences, this knowledge will never be truly verified. We won’t even attempt to get to the bottom of it in class; instead, we’ll practice some basic methods of computer simulation for statistical inference and for generating data that has some idealised characteristics. Such methods play an increasingly important role in computational statistics and are extremely useful for designing robust data collection and analysis plans. If you make a mistake in the code and end up in an infinite loop, but you’re afraid that stopping the process may cause the known universe to implode, you can watch Dick on YouTube while you wait. If something like this can happen to our data, who says it couldn’t happen to us?\n\n\n\n\nWeek 6  Challenging hierarchies  Multilevel models\n\n\nBy now we got a sense that every new thing we learn about turns out to be merely a specific case of a larger class of things. So, all the models we covered so far are specific, single-level, versions of multilevel models, in which our cases can be seen as clustered within larger entities. Sometimes they are part of several cross-cutting clusters and/or the clusters are themselves clustered. In general terms, we must acknowledge that there are dependencies in our data that may influence their behaviour. It turns out that data about humans living in societies look somewhat like humans living in societies. The importance of including information about hierarchical dependencies in our models is probably emphasised by no one else more than McElreath (2020:15), who wants “to convince the reader of something that appears unreasonable: multilevel regression deserves to be the default form of regression. Papers that do not use multilevel models should have to justify not using a multilevel approach.” We will encounter some of the uses and challenges of multilevel modelling.\n\n\n\n\nWeek 7  The unobserved  Latent variables and structural models\n\n\nThe unobserved sounds like the title of a promising horror film; if we have achieved our aims in the module so far, our horror should be ‘merely’ metaphysical by now (Kołakowski anyone? No? Okay, never mind). We have already had to deal with various aspects of latency in our analyses. At the most fundamental level, we speak about population parameters, but we never actually observe them; even a sample statistic can be a purely imaginary case that doesn’t occur in real life. We have discussed the effects of omitted variables, which are thus unobserved by our model, but which we may have access to in our data. And, of course, our most interesting measurements are likely to be proxies of some unobservable theoretical construct (Mulvin (2021) has recently published a wonderfully rich book about proxies in general). This week we pick up an earlier thread from week 4, where we thought about binary and ordered multinomial variables as discretised manifestations of some continuous ‘latent variable’. We expand on this idea by exploring simple and then more complex latent variable models (factor analysis, structural equation modelling), as a further generalisation of the hierarchical perspective introduced earlier. This gives us a few more tools to deal with our radical uncertainty. (n.b. missing data points are another challenge that could fall under this heading, and learning how to deal with them is extremely important; but “The missing” is too good a title not to deserve a high-budget, weak-storyline, full-on special effects sequel somewhere else)\n\n\n\n\nWeek 8  Words, words, mere words…  Text as data\n\n\nAs researchers in humanities and the social sciences, we use words both as tools of analysis and as sources of data. Words, and more broadly, texts, are also increasingly important for quantitative research in an age of so-called ‘big data’, when the digital world is saturated with unstructured textual information. But the statistical inspection of text is neither new, nor restricted to the humanistic tail of the social sciences. For example, a documented interest in the statistical study of literary style for the purposes of attributing authorship dates back to the mid-1850s (see El-Shagi and Jung 2015); and investors can use textual data such as minutes from the Bank of England’s Monetary Policy Committee’s deliberations to estimate future monetary policy decisions before they are actually taken (cf. Lord 1958). Methods for the collection and quantitative analysis of large-scale textual data are increasingly available, but their technical implementation is complex and requires efficient combination of humanistic subject knowledge and statistical expertise. Faced with words, one is understandably caught between Shakespeare’s Troilus and Wilde’s Dorian Gray. “Words, words, mere words, no matter from the heart; th’ effect doth operate another way. … My love with words and errors still she feeds, but edifies another with her deeds” - believed the betrayed Troilus. “Words! Mere words! How terrible they were! How clear, and vivid, and cruel! One could not escape from them. And yet what a subtle magic there was in them! They seemed to be able to give a plastic form to formless things, and to have a music of their own as sweet as that of viol or of lute. Mere words! Was there anything so real as words?” - pondered Dorian.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nDavid, F. N. 1955. “Studies in the History of Probability and Statistics i. Dicing and Gaming (a Note on the History of Probability).” Biometrika 42(1/2):1–15. doi: 10.2307/2333419.\n\n\nEl-Shagi, Makram, and Alexander Jung. 2015. “Have Minutes Helped Markets to Predict the MPC’s Monetary Policy Decisions?” European Journal of Political Economy 39:222–34. doi: 10.1016/j.ejpoleco.2015.05.004.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and other stories. Cambridge: Cambridge University Press.\n\n\nLord, R. D. 1958. “Studies in the History of Probability and Statistics.: VIII. De Morgan and the Statistical Study of Literary Style.” Biometrika 45(1/2):282–82. doi: 10.2307/2333072.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. Boca Raton: Taylor and Francis, CRC Press.\n\n\nMulvin, Dylan. 2021. Proxies: The Cultural Work of Standing in. Cambridge, Massachusetts: The MIT Press.\n\n\nSenn, Stephen. 2003. “A Conversation with John Nelder.” Statistical Science 18(1):118–31. doi: 10.1214/ss/1056397489."
  },
  {
    "objectID": "materials/slides-frame/slides-frame_w02.html",
    "href": "materials/slides-frame/slides-frame_w02.html",
    "title": "Week 2  Revisiting Flatland",
    "section": "",
    "text": "View the slides full-screen in a standalone browser window here. The lecture recording is available on ReCap (requires Newcastle University login)\n\n\n\n\n\nReferences\n\nDavid, F. N. 1955. “Studies in the History of Probability and Statistics i. Dicing and Gaming (a Note on the History of Probability).” Biometrika 42 (1/2): 1–15. https://doi.org/10.2307/2333419.\n\n\nEl-Shagi, Makram, and Alexander Jung. 2015. “Have Minutes Helped Markets to Predict the MPC’s Monetary Policy Decisions?” European Journal of Political Economy 39 (September): 222–34. https://doi.org/10.1016/j.ejpoleco.2015.05.004.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and other stories. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nLord, R. D. 1958. “Studies in the History of Probability and Statistics.: VIII. De Morgan and the Statistical Study of Literary Style.” Biometrika 45 (1/2): 282–82. https://doi.org/10.2307/2333072.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. CRC Texts in Statistical Science. Boca Raton: Taylor and Francis, CRC Press.\n\n\nMulvin, Dylan. 2021. Proxies: The Cultural Work of Standing in. Infrastructures Series. Cambridge, Massachusetts: The MIT Press.\n\n\nSenn, Stephen. 2003. “A Conversation with John Nelder.” Statistical Science 18 (1): 118–31. https://doi.org/10.1214/ss/1056397489."
  },
  {
    "objectID": "data/index.html",
    "href": "data/index.html",
    "title": "Data documentation",
    "section": "",
    "text": "The datasets listed in the table below can be read into R from \"https://cgmoreh.github.io/HSS8005-data/___\" (using a type-appropriate read function and replacing ___ with “File name” and “Type” extension; e.g. dataset <- haven::read_dta(\"https://cgmoreh.github.io/HSS8005-data/dataset.dta\")).\n\n\n\n\n\n\n\n\n\nFile name\n\n\nOriginal name\n\n\nType\n\n\nVersion\n\n\nOrigin\n\n\nAccess\n\n\n\n\n\n\nosterman\n\n\nReplication_data_ESS1-9_20201113\n\n\n.dta\n\n\nNA\n\n\nReplication data for Österman (2021), based on European Social Survey Rounds 1-9 data\n\n\nSource\n\n\n\n\nLaddLenz\n\n\nLaddLenz\n\n\n.dta\n\n\nNA\n\n\nReplication data for Ladd and Lenz (2009), based on British Election Panel Study data. Included in Hainmueller (2012)\n\n\nSource\n\n\n\n\n\n\n\n\n\n\nReferences\n\nHainmueller, Jens. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n\nLadd, Jonathan McDonald, and Gabriel S. Lenz. 2009. “Exploiting a Rare Communication Shift to Document the Persuasive Power of the News Media.” American Journal of Political Science 53 (2): 394–410. https://doi.org/10.1111/j.1540-5907.2009.00377.x.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Second. CRC Texts in Statistical Science. Boca Raton: Taylor and Francis, CRC Press.\n\n\nÖsterman, Marcus. 2021. “Can We Trust Education for Fostering Trust? Quasi-experimental Evidence on the Effect of Education and Tracking on Social Trust.” Social Indicators Research 154 (1): 211–33. https://doi.org/10.1007/s11205-020-02529-y."
  },
  {
    "objectID": "materials/worksheets/worksheets_w02.html",
    "href": "materials/worksheets/worksheets_w02.html",
    "title": "Week 2 Computer Lab Worksheet",
    "section": "",
    "text": "This session introduces simple and multiple linear regression models. You will be working with data from Österman (2021) to replicate parts their analysis. We will be covering only basic regression methods in this session, so the article will serve mainly as a broad background to the data here. We will be returning to this article in future weeks too, expanding our modelling strategy as we discover new methods. We will also practice some data management tasks and the basics of data visualisation using principles from ‘the grammar of graphics’ as implemented in the ggplot2() package (see Kieran Healy’s Data Visualization: A practical introduction for an introduction with many practical examples).\nBy the end of the session, you will:\n\nlearn how to import data from foreign formats (e.g. SPSS, Stata, CSV)\nknow how to perform basic descriptive statistics on a dataset\nunderstand the basics of data visualisation\nknow how to fit linear regression models in R using different functions\nlearn a few options for presenting findings from regression models"
  },
  {
    "objectID": "materials/worksheets/worksheets_w02.html#setup",
    "href": "materials/worksheets/worksheets_w02.html#setup",
    "title": "Week 2 Computer Lab Worksheet",
    "section": "Setup",
    "text": "Setup\nIn Week 1 you set up R and RStudio, and an RProject folder (we called it “HSS8005_labs”) with an .R script and a .qmd or .Rmd document in it (we called these “Lab_1”). Ideally, you saved this on a cloud drive so you can access it from any computer (e.g. OneDrive). You will be working in this folder. If it’s missing, complete Task 2 from the Week 1 Lab.\nYou can create a new .R script and .qmd/.Rmd for this week’s work (e.g. “Lab_2”). Start working in the .R script initially, then switch to .qmd/.Rmd later in the session to report on your final analysis."
  },
  {
    "objectID": "materials/worksheets/worksheets_w02.html#importing-data",
    "href": "materials/worksheets/worksheets_w02.html#importing-data",
    "title": "Week 2 Computer Lab Worksheet",
    "section": "Importing data",
    "text": "Importing data\nAs we have seen in Week 1, small datasets that are included in R packages (including base R) for demonstration purposes can be used by simply invoking the name of the dataset. For example, the command head(mtcars) would print out the first 5 rows (cases) in the “mtcars” dataset included in base R (more specifically, in its “datasets” package):\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe data() function lists all the built-in packages. You can further specify package = to list datasets included in a given package; e.g.:\n\ndata(package = \"dplyr\")\n\n# Note that {dplyr} is one of the data management packages included in the core {tidyverse}. Make sure the {tidyverse} is installed.\n\n\n\nWe can also import built-in datasets to our Environment in order to inspect them manually:\n\nmtcars_data <- mtcars\n\nIf we want to access a dataset from a non-base-R package, we need to ensure that the package is installed on our system and that we specify the name of the package; e.g.:\n\nhead(starwars) # gives an Error\n\nError in head(starwars): object 'starwars' not found\n\nhead(dplyr::starwars)\n\n# A tibble: 6 × 14\n  name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n  <chr>         <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n2 C-3PO           167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n3 R2-D2            96    32 <NA>    white,… red        33   none  mascu… Naboo  \n4 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n5 Leia Organa     150    49 brown   light   brown      19   fema… femin… Aldera…\n6 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n# … with 4 more variables: species <chr>, films <list>, vehicles <list>,\n#   starships <list>, and abbreviated variable names ¹​hair_color, ²​skin_color,\n#   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n  # works, as long as {dplyr} or the whole {tidyverse} are installed\n\nReal-life datasets, however, need to be imported into R. Datasets come in various formats. R’s native data format has the extension .rds and can be imported with the readRDS() function. The counterpart function saveRDS() exports a dataset to the .rds format. The core-tidyverse {readr} package has similar functions (read_rds() / write_rds()).\nThe .rds format is useful because it can be compressed to various sizes to take up less space, but can only be directly opened in R. It is much more common to encounter data saved in a “delimited” text format, which can be easily opened in a spreadsheet viewer/editor such as Excel. This makes it very interchangeable and therefore very common. The most common is probably the “comma separated values” (.csv) format, which can be imported with the base-R function read.csv() or the tidyverse readr::read_csv() equivalent. Read Chapter 11 in R4DS for more on these functions.\nVery often, you will need to import data saved in the format of another proprietary statistical analysis package such as SPSS or Stata. Large survey projects usually make data available in these formats. The great advantage of these formats is that they can incorporate additional information about variables and the levels of categorical variables (e.g. value labels, specific values for different types of missing values). These additional information can be extremely valuable, but they are not handled straight-forwardly in text-based format, spreadsheets and R’s native data format. To make them operational in R, we need a few specially designed functions.\nThe {haven} package — part of the extended {tidyverse}, meaning that it is installed on your system as part of {tidyverse}, but the library(\"tidyverse\") command does not load it by default; it needs to be loaded explicitly — is one of the most commonly used for this purpose. Functions such as read_sas(), read_sav() and read_dta() import datasets specific to the SAS, SPSS and Stata programs, respectively.\nIt is highly recommended to read the documentation available for the {haven} package to understand how it operates. Fundamentally, it is designed to import data to a intermediary format which stores the additional labeling information in a special format that allows users to access them, but not making them easy to use directly. A suite of packages developed by Daniel Lüdecke from the University of Hamburg offer some additional functionality to work with labels directly when summarising and plotting data. These packages integrate well with the {tidyverse} and are actively maintained, and we will use them in this course to make our lives a bit easier. To install them, run:\n\n# We can install several packages at once by first creating a vector of their names\n\nsj_packages <- c(\"sjlabelled\", \"sjPlot\", \"sjstats\", \"ggeffects\", \"sjmisc\")\n\ninstall.packages(sj_packages)\n\nThe functions sjlabelled::read_sas(), sjlabelled::read_spss() and sjlabelled::read_stata() are the {sjlabelled} equivalents of the {haven} functions mentioned above. This vignette article included with the package explains the main differences between the two.\nAs a first step, let’s import the osterman dataset that underpins the Österman (2021) article (see the Data page on the course website for information about the datasets available in this course):\n\nosterman <- sjlabelled::read_stata(\"https://cgmoreh.github.io/HSS8005-data/osterman.dta\")\n\n\nUsing functions learnt in Week 1, do the following:\n\ncheck the dimensions of the dataset; what does it tell you?\nprint a summary of the entire dataset; what have you learnt about the data?\n\n\nA very convenient way to create a codeplan for a dataset – especially if it has value-labelled categorical variables – is offered by the sjPlot::view_df() function, which produces a tables in HTML format that can be saved and consulted to get more information about the variables. With the default settings, we get the following:\n\nsjPlot::view_df(osterman)\n\nThe output opens up in the Viewer pane.\nThere are several additional options that can add useful complexity to the codeplan, as well as the option to restrict it to selected variables. It also works in a “piper” workflow, so it can be combined with {dplyr} verbs such as select to restrict variables beforehand in a more flexible way. Below we request a codeplan with extended options:\n\nsjPlot::view_df(osterman,\n                show.na = TRUE, \n                show.type = TRUE, \n                show.frq = TRUE, \n                show.prc = TRUE, \n                show.string.values = TRUE)\n\nThe output can also be opened in an external web browser window by clicking on the third (last) icon at the top of the Viewer toolbar. From the browser window, with a Ctrl + Right-click > Save as... we can save the table as an HTML document locally and use it as a reference.\n\nBefore moving forward, spend some time examining the codeplan that you have produced and if you haven’t yet had a chance to skim through the Österman (2021) article, have a quick read through their description of the dataset."
  },
  {
    "objectID": "materials/worksheets/worksheets_w02.html#exercise-1-create-some-basic-descriptive-graphs-using-the-ggplot-command-from-the-ggplot2-tidyverse-package-for-the-associaton-between-the-following-variables",
    "href": "materials/worksheets/worksheets_w02.html#exercise-1-create-some-basic-descriptive-graphs-using-the-ggplot-command-from-the-ggplot2-tidyverse-package-for-the-associaton-between-the-following-variables",
    "title": "Week 2 Computer Lab Worksheet",
    "section": "Exercise 1: Create some basic descriptive graphs using the ggplot() command from the {ggplot2} tidyverse package for the associaton between the following variables:",
    "text": "Exercise 1: Create some basic descriptive graphs using the ggplot() command from the {ggplot2} tidyverse package for the associaton between the following variables:\n\n‘trustindex3’ and ‘eduyrs25’\n‘trustindex3’ and ‘agea’\n‘trustindex3’ and ‘female’"
  },
  {
    "objectID": "materials/worksheets/worksheets_w02.html#exercise-2-what-factors-affect-trust",
    "href": "materials/worksheets/worksheets_w02.html#exercise-2-what-factors-affect-trust",
    "title": "Week 2 Computer Lab Worksheet",
    "section": "Exercise 2: What factors affect trust?",
    "text": "Exercise 2: What factors affect trust?\nFit three simple bivariate OLS regressions using the lm() function:\n\nRegress ‘trustindex3’ on ‘eduyrs25’ and interpret the results\nRegress ‘trustindex3’ on ‘agea’ and interpret the results\nRegress ‘trustindex3’ on ‘female’ and interpret the results\nRegress ‘trustindex3’ on all three predictors listed above and interpret the results"
  },
  {
    "objectID": "materials/worksheets/worksheets_w02.html#advanced-exercise-3-apply-the-model-to-a-new-dataset",
    "href": "materials/worksheets/worksheets_w02.html#advanced-exercise-3-apply-the-model-to-a-new-dataset",
    "title": "Week 2 Computer Lab Worksheet",
    "section": "(Advanced) Exercise 3: Apply the model to a new dataset",
    "text": "(Advanced) Exercise 3: Apply the model to a new dataset\nThe ostermann data originates from Waves 1-9 of the European Social Survey. The ESS data are accessible freely upon registration. As part of this exercise, access data from Wave 10 of the survey (from this site: https://www.europeansocialsurvey.org/data/) and perform the following tasks:\n\ndownload the dataset to the Rproject folder\nselect the variables required to recreate the data to fit the multiple regression model from the previous exercise\ncreate your version of the ‘trustindex3’ variable\nfit the models from Exercise 2 and copare the results.\n\nYou should already be familiar with the functions needed to complete each of these steps, but it may require some self-study. You will likely need to continue the task outside class. If you succeed in complating the Task, in Week 3 we can compare results."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HSS8005 {{< iconify line-md plus >}}",
    "section": "",
    "text": "Quantitative analysis \n        \n        \n            HSS8005 • Intermediate/Advanced stream • 2023\nNewcastle University (UK)\n        \n        \n            A second course in applied statistics and probability for the understanding of society and culture. It is aimed at an interdisciplinary audience through real-life research examples from various fields in the social sciences and humanities. The course emphasizes the scientific application of statistical methods, developing a reproducible research workflow, and computational techniques.\n        \n    \n\n\n\n\n\nModule leader\n\n   Dr. Chris Moreh\n   HDB.4.106\n   chris.moreh@newcastle.ac.uk\n   Tutorial booker\n \n\n\n\nTeaching Assistants\n\n   Bilal Alsharif\n   Fengting Du\n\n\n\n\n\nSession dates\n\n   Thursdays\n    Check on your Timetable app\n   Lecture: 10:00-11:30\n   Labs:   13:00-14:30 (Group 03)        14:30-16:00 (Group 04)  \n\n\n\nAssessment\n\n   26th April 2023\n   3,500-word long research report\n   Submit to Turnitin via Canvas\n\n\n\n\n Chris’s mastodon feed\nwhere he posts stuff of interest to #HSS8005\n\n\n\n\n\n\nModule overview\nThis module is offered by School X - Researcher Education and Development to postgraduate students within the Faculty of Humanities and Social Sciences at Newcastle University. The module aims to provide a broad applied introduction to more advanced methods in quantitative analysis for students from various disciplinary backgrounds. See the module plan page for details about the methods covered. The course content consists of eight lectures (1.5 hours each) and eight IT labs (1.5 hours) . The course stands on three pillars: application, reproducibility and computation.\nApplication: we will work with real data originating from large-scale representative surveys or published research, with the aim of applying methods to concrete research scenarios. IT lab exercises will involve reproducing small bits of published research, using the data and (critically) the modelling approaches used by the authors. The aim is to see how methods have been used in practice in various disciplines and learn how to reproduce (and potentially improve) those analyses. This will then enable students to apply this knowledge to their own research questions. The data used in IT labs may be cleansed to allow focusing more on modelling tasks than on data wrangling, but exercises will address some of the more common data manipulation challenges and will cover essential functions. Data cleansing scripts will also be provided so that interested students can use them in their own work.\nReproducibility: developing a reproducible workflow that allows your future self or a reviewer of your work to understand your process of analysis and reproduce your results is essential for reliable and collaborative scientific research. We enforce the ideas and procedures of reproducible research both through replicating published research (see above) and in our practice (in the IT labs and the assignment). For an overview of why it’s important to develop a reproducible workflow early on in your research career and how to do it using (some) of the tools used in this module, read Chapter 3 of TSD (see Resources>Readings). It’s also worth reading through Kieran Healy’s The Plain Person’s Guide to Plain Text Social Science, although there are now better software options than those discussed there. In this course, we will be using a suite of well-integrated free and open-source software to aid our reproducible workflow: the  statistical programming language and its currently most popular dialect – the {tidyverse} – via the  IDE for data analysis, and  for scientific writing and publishing (see Resources>Software).\nComputation: the development of computational methods underpins the application of the most important statistical ideas of the past 50 years (see Andrew Gelman’s article on these developments here or an online workshop talk here; Richard McElreath’s great talk on Science as Amateur Software Development is well worth watching too). This module aims to develop basic computational skills that allow the application of complex statistical models to practical scientific problems without advanced mathematical knowledge, and which lay the foundation on which students can then pursue further learning and research in computational humanities and social sciences.\n\nThe course and the website were written and are maintained by Chris Moreh. The source-code is available on  GitHub\n\n\nPrerequisites\nTo benefit the most from this module, students are expected to have a foundational level of knowledge in quantitative methods: a good understanding of data types and distributions, familiarity with inferential statistics, and some exposure to linear regression. This is roughly equivalent to the content covered in the Introductory stream of the module or a textbook such as OpenIntro Statistics (which you can download for free in PDF).\nThose who don’t feel completely up to date with linear regression but are determined to advance more quickly and read/practice beyond the compulsory material during weeks 1-3 are also encouraged to sign up.\nThose with a stronger background in multiple linear regression (e.g. students with undergraduate-level training in econometrics) will still benefit from weeks 1-3 as the approach we are taking is probably different from the one they are familiar with.\nNo previous knowledge of  or command-based statistical analysis software is needed. Gaining experience with using statistical software is part of the skills development aims of the module. However, it is not a general data science module, and the IT labs will cover a very limited number of functions (from both base R, the tidyverse and other reliable user-written packages) that are most useful for tackling specific analysis tasks. Students are advised to complete some additional self-paced free online training in the use of the software, such as Data Carpentry’s R for Social Scientists, and to consult Wickham, Çetinkaya-Rundel and Grolemund’s R for Data Science (2nd ed.) online book."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "HSS8005 {{< iconify line-md plus >}}",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "teachingResources.html",
    "href": "teachingResources.html",
    "title": "Readings",
    "section": "",
    "text": "Regression and Other Stories\nTelling Stories with Data\nMaximum Likelihood for Social Science\nBeyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R"
  },
  {
    "objectID": "teachingResources.html#history-of-statistics",
    "href": "teachingResources.html#history-of-statistics",
    "title": "Readings",
    "section": "History of statistics",
    "text": "History of statistics\n\nGelman and Vehtari: What are the most important statistical ideas of the past 50 years?\n(Gelman and Vehtari 2021)\nLaplace’s Demon: https://www.stsci.edu/~lbradley/seminar/laplace.html\n\nHistory of games of chance and their relationship to divinity: https://www.britannica.com/topic/gambling/History; https://www.pokcas.com/the-origins-of-games-of-chance-how-gambling-has-evolved-over-5000-years/\nImage of Fisher’s crop fields: https://www.adelaide.edu.au/library/special/exhibitions/significant-life-fisher/rothamsted/Latin-squares_Beddgelert.png\nImage of Messi’s face in Argentine corn field: https://s2.reutersmedia.net/resources/r/?m=02&d=20230118&t=2&i=1620494870&w=&fh=545&fw=810&ll=&pl=&sq=&r=LYNXMPEJ0H0MG"
  },
  {
    "objectID": "teachingResources.html#general-and-generalised-linear-models",
    "href": "teachingResources.html#general-and-generalised-linear-models",
    "title": "Readings",
    "section": "General and Generalised Linear models",
    "text": "General and Generalised Linear models\n\nI found this page on The General Linear Model (GLM) quite easy to read.\nAnd this one on [Generalised Linear Models](https://www.mygreatlearning.com/blog/generalized-linear-models/#:~:text=Generalized%20Linear%20Model%20(GLiM%2C%20or,other%20than%20a%20normal%20distribution.):\n\nWhat is a Generalized Linear Model?\nGeneralized Linear Model (GLiM, or GLM) is an advanced statistical modelling technique formulated by John Nelder and Robert Wedderburn in 1972. It is an umbrella term that encompasses many other models, which allows the response variable y to have an error distribution other than a normal distribution. The models include Linear Regression, Logistic Regression, and Poisson Regression.\nIn a Linear Regression Model, the response (aka dependent/target) variable ‘y’ is expressed as a linear function/linear combination of all the predictors ‘X’ (aka independent/regression/explanatory/observed variables). The underlying relationship between the response and the predictors is linear (i.e. we can simply visualize the relationship in the form of a straight line). Also, the error distribution of the response variable should be normally distributed. Therefore we are building a linear model.\nGLM models allow us to build a linear relationship between the response and predictors, even though their underlying relationship is not linear. This is made possible by using a link function, which links the response variable to a linear model. Unlike Linear Regression models, the error distribution of the response variable need not be normally distributed. The errors in the response variable are assumed to follow an exponential family of distribution (i.e. normal, binomial, Poisson, or gamma distributions). Since we are trying to generalize a linear regression model that can also be applied in these cases, the name Generalized Linear Models."
  },
  {
    "objectID": "teachingResources.html#simulation-and-study-design",
    "href": "teachingResources.html#simulation-and-study-design",
    "title": "Readings",
    "section": "Simulation and study design",
    "text": "Simulation and study design\n\nhttps://iqss.github.io/prefresher/simulation.html#fnref27\nReinhart: RegressinatorR package\nData simulation in the context of multilevel regression and post-stratification: https://tellingstorieswithdata.com/15-mrp.html#simulation\nDesign and Analysis of Experiments and Observational Studies using R\nhttps://julianquandt.com/post/power-analysis-by-data-simulation-in-r-part-i/\nThe Survey Quality Predictor (SQP)"
  },
  {
    "objectID": "teachingResources.html#causality",
    "href": "teachingResources.html#causality",
    "title": "Readings",
    "section": "Causality",
    "text": "Causality\n\nDAGs: Deffner, Dominik, Julia M. Rohrer, and Richard McElreath. 2022. ‘A Causal Framework for Cross-Cultural Generalizability’. Advances in Methods and Practices in Psychological Science 5(3):25152459221106370. doi: 10.1177/25152459221106366."
  },
  {
    "objectID": "teachingResources.html#network-analysis",
    "href": "teachingResources.html#network-analysis",
    "title": "Readings",
    "section": "Network analysis",
    "text": "Network analysis\n\nhttps://ladal.edu.au/net.html\nhttps://bookdown.org/jdholster1/idsr/network-analysis.html"
  },
  {
    "objectID": "teachingResources.html#hierarchies",
    "href": "teachingResources.html#hierarchies",
    "title": "Readings",
    "section": "Hierarchies",
    "text": "Hierarchies\n\nSadler, Michael E., and Christopher J. Miller. 2010. ‘Performance Anxiety: A Longitudinal Study of the Roles of Personality and Experience in Musicians’. Social Psychological and Personality Science 1(3):280–87. doi: 10.1177/1948550610370492.\n\ndata: musicdata.csv\nsource: https://github.com/proback/BeyondMLR/tree/master/data\nusage: https://bookdown.org/roback/bookdown-BeyondMLR/ch-multilevelintro.html#cs:music\n\nShakespeare, Austen, Bronte, Dickens data used in the context of multilevel modelling: https://tellingstorieswithdata.com/15-mrp.html#austen-bront%C3%AB-dickens-and-shakespeare\nDeffner, Dominik, Julia M. Rohrer, and Richard McElreath. 2022. ‘A Causal Framework for Cross-Cultural Generalizability’. Advances in Methods and Practices in Psychological Science 5(3):25152459221106370. doi: 10.1177/25152459221106366. - also use multilevel regression with post-stratification"
  },
  {
    "objectID": "teachingResources.html#timelines",
    "href": "teachingResources.html#timelines",
    "title": "Readings",
    "section": "Timelines",
    "text": "Timelines\n\nBernal, James Lopez, Steven Cummins, and Antonio Gasparrini. 2017. ‘Interrupted Time Series Regression for the Evaluation of Public Health Interventions: A Tutorial’. International Journal of Epidemiology 46(1):348–55. doi: 10.1093/ije/dyw098.\n\ndata: sicily.rda\nsource: https://hbiostat.org/data/\nusage scripts: http://hbiostat.org/rmsc/genreg.html#complex-curve-fitting-example"
  },
  {
    "objectID": "teachingResources.html#spacecraft",
    "href": "teachingResources.html#spacecraft",
    "title": "Readings",
    "section": "Spacecraft",
    "text": "Spacecraft\n\nSpatial regression models\nSpatial Data Science"
  },
  {
    "objectID": "teachingResources.html#text-as-data",
    "href": "teachingResources.html#text-as-data",
    "title": "Readings",
    "section": "Text as data",
    "text": "Text as data\n\nMere words - Dorain Gray: https://www.gutenberg.org/files/174/174-h/174-h.htm#chap02\nMere words - Troilus: http://shakespeare.mit.edu/troilus_cressida/troilus_cressida.5.3.html\n\nText Mining with R: A Tidy Approach (Dissferent Austen data used: https://www.tidytextmining.com/tfidf.html)\nhttps://tellingstorieswithdata.com/16-text.html\nChris Bail: https://cbail.github.io/textasdata/"
  },
  {
    "objectID": "teachingResources.html#classification-ml",
    "href": "teachingResources.html#classification-ml",
    "title": "Readings",
    "section": "Classification, ML",
    "text": "Classification, ML\n\nhttps://towardsdatascience.com/machine-learning-techniques-for-investigative-reporting-344d74f69f84 - a very interesting data analysis on Brexit voting prediction; unfortunately the dataset cannot be located, but may be reproducible from other sources."
  },
  {
    "objectID": "teachingResources.html#intuition-building",
    "href": "teachingResources.html#intuition-building",
    "title": "Readings",
    "section": "Intuition building",
    "text": "Intuition building\n\nHamming, Richard W. 1997. The Art of Doing Science and Engineering: Learning to Learn. Amsterdam: Gordon and Breach.\nhttps://data-feminism.mitpress.mit.edu/\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Boca Raton: Taylor and Francis, CRC Press. - First two chapters freely available\nSmoking debate:\n\nSee “The book of Why” for the story of how Fisher in particular was involved in the sceptical resistance to linking tobacco with cancer\nAlex Reinhart: The history of “How to Lie with Smoking Statistics” - Daryll Huff’s involvement.\nSee also: Gelman, Andrew. 2012. ‘Ethics and Statistics: Statistics for Cigarette Sellers’. CHANCE 25(3):43–46. doi: 10.1080/09332480.2012.726563\n\nReinhart: Statistics done wrong"
  },
  {
    "objectID": "teachingResources.html#r-style-guides",
    "href": "teachingResources.html#r-style-guides",
    "title": "Readings",
    "section": "R style guides",
    "text": "R style guides\n\nhttps://style.tidyverse.org/\nhttps://google.github.io/styleguide/Rguide.html"
  },
  {
    "objectID": "teachingResources.html#project-management",
    "href": "teachingResources.html#project-management",
    "title": "Readings",
    "section": "Project management",
    "text": "Project management\n\nhttps://web.stanford.edu/~gentzkow/research/CodeAndData.pdf\nhttps://learningds.org/ch/01/lifecycle_intro.html\nReproducible, transparent, credible research: https://worldbank.github.io/dime-data-handbook/\nQuarto for Scientists"
  },
  {
    "objectID": "teachingResources.html#data-sources",
    "href": "teachingResources.html#data-sources",
    "title": "Readings",
    "section": "Data sources",
    "text": "Data sources\n\nhttps://data.london.gov.uk/\nhttps://i4replication.org/reports.html\nAlgorithmic Bias - full data and code\nhttps://economics.mit.edu/people/faculty/josh-angrist/angrist-data-archive\nhttps://economics.mit.edu/people/faculty/josh-angrist/mhe-data-archive\nNorris, Pippa, 2020, “Global Party Survey, 2019”\nLarge-scale cross-cultural prosocial behaviour study:\n\nHouse, Bailey R., Patricia Kanngiesser, H. Clark Barrett, Tanya Broesch, Senay Cebioglu, Alyssa N. Crittenden, Alejandro Erut, Sheina Lew-Levy, Carla Sebastian-Enesco, Andrew Marcus Smith, Süheyla Yilmaz, and Joan B. Silk. 2020. ‘Universal Norm Psychology Leads to Societal Diversity in Prosocial Behaviour and Development’. Nature Human Behaviour 4(1):36–44. doi: 10.1038/s41562-019-0734-z.\nData analyised by (Deffner, Rohrer, and McElreath 2022)\n\nSimpson’s paradox:\n\nBickel, P. J., E. A. Hammel, and J. W. O’Connell. 1975. ‘Sex Bias in Graduate Admissions: Data from Berkeley’. Science 187(4175):398–404. doi: 10.1126/science.187.4175.398.\n\nhere - here\n\n\nLEGO bricksets:\n\nPeterson, Anna D., and Laura Ziegler. 2021. ‘Building a Multiple Linear Regression Model With LEGO Brick Data’. Journal of Statistics and Data Science Education 29(3):297–303. doi: 10.1080/26939169.2021.1946450.\nData and code: C:\\Users\\moreh\\OneDrive - Newcastle University\\DATA\\Peterson, Ziegler (2021) LEGO data"
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "HSS8005 {{< iconify line-md plus >}}",
    "section": "",
    "text": "ReadingsSoftwareTrainingHelpOther\n\n\n\nTextbooks\nThe course does not strictly follow the content of a textbook, but the expectation is that students will read as much as possible of the assigned chapters from the following books:\n\n\n\n\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and other stories. Cambridge: Cambridge University Press.  ROS\n\n\nFree to download PDF version from the book’s website: https://avehtari.github.io/ROS-Examples\n\n\n\n\n\n\n\nAlexander, Rohan. 2023. Telling Stories with Data. Chapman and Hall/CRC  TSD\n\n\nFree online book: https://tellingstorieswithdata.com\n\n\n\n\n\n\n\nGelman, A., and Hill, J. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge: Cambridge University Press.  ARM\n\n\nNote: ROS is the expanded and updated version of Part 1 (and some of Part 3) of this book. While everyone in the free world eagerly awaits the publication of ROS’s multilevel counterpart, we’ll use ARM as a reference work for the theory underpinning multilevel modelling.  Not freely available. Access it in print or online via the NU library\n\n\n\n\n\n\nRelatively large portions of text will be assigned for reading in each week from these books, referring to them by their acronyms. Don’t worry if you cannot read all the textbook content assigned in any given week! Those for whom the method covered by the assigned readings is new, will be able to refer back to them throughout the semester and beyond, reading thoroughly and completing the applied exercises. Those already familiar to some extent with the methods, should nonetheless read the text as a narrative and will discover hidden gems that will spectacularly improve their understanding and ability to interpret their statistical results.\n\n\nApplication\nIn the IT labs we will practice applying methods by reproducing small bits of published research, using the data and (critically) the modelling approaches used by the authors. To fully understand the context of these data and the methods used, you must read the original journal articles and the available supplementary materials provided alongside. These readings will be listed under each week’s outline (still work in progress!).\nThe articles come from a variety of different fields, so expect them to push you outside your disciplinary comfort zone. The point is to see how methods have been used in practice and learn how to reproduce (and potentially improve) those analyses. This will then enable you to apply this knowledge to your own research questions.\nWhen selecting the articles, the aim was to strike a fine balance between (a) the simplicity of the methods employed, (b) data and analytical transparency, and (c) the strength of the analysis. So don’t take them as examples of all-rounds best practice, but examples of research that gets published while being self-confident enough to open itself up for public scrutiny. Aim for this in your own research!\n\n\nTechnique\nThere will also be various readings relating more closely to the technicalities of coding in R and scientific writing, collaboration and communication in general. These readings will also be listed under each week’s outline as the semester progresses. The generic reading that students are advised to go through on their own is:\n\n\n\n\n\n\n\n\nWickham, Çetinkaya-Rundel and Grolemund. 2022. R for Data Science (2nd ed.)  R4DS\n\n\nFree online book: https://r4ds.hadley.nz/\n\n\n\n\n\n\n\nIntuition\nFinally, there will also be recommended readings listed under certain weeks that help place methods, statistics and probability theory in a broader frame. These are useful readings for everyone, regardless of whether you will be applying quantitative analysis in your research or future work.\n\n\n\n\nRequired software\nWe will use a number of open-source software for data analysis and scientific writing. You need to install these on your personal computers to be able to work away from campus:\n\n\n\n\nR\n(programming language)\nEssential\nR needs to be installed even if we will only use it via the RStudio interface.\nInstall the latest version from here\n\n\n\nRStudio\n(integrated development environment)\nEssential\nYou will need the free desktop version appropriate for your operating system. RStudio combines the R Console - the direct interface to R - with a number of other panels.\nInstall the latest version from here\n\n\n\nTidyverse\n(collection of R packages)\nEssential\nThe tidyverse is a collection of packages that make the R language easier to use by introducing a more consistent grammar. It provides functions that are particularly useful for data manipulation and visualisation. It is the most common ‘dialect’ used among social scientists.\nInstall from within RStudio by executing in the Console:\ninstall.packages(\"tidyverse\")\n\n\n\nQuarto\n(scientific publishing system)\nEssential\nWe will be using Quarto markdown documents (.qmd) throughout the course to document our data analysis. .qmd files extend the plain-text Markdown mark-up language (.md) to allow for data analysis code to be executed and results presented alongside the main text. This is an essential requirement for analytical transparency, reliability and reproducibility.The assignment will also be completed in .qmd.\nIncluded by default in the latest RStudio release; no need to install separately.\nYou can check your installation by executing in the RStudio Terminal :\nquarto check\n\n\n\nZotero\n(reference manager)\nRecommended\nIf you are not yet using a reference manager, I recommend giving Zotero a try. It will make your work much more efficient and it integrates (relatively well) with RMarkdown and Quarto using the the Better BibTeX add-on.\nInstall the latest version and add-ons from here\n\n\n\n\n\n\nStudents with no previous experience using R and/or RStudio are advised to complete the self-paced free online training course R for Social Scientists provided by Data Carpentry at https://datacarpentry.org/r-socialsci/\n\n\n\nThere are several ways to get help with R outside class. If you encounter an error message or are looking for a function to perform a specific task that we have not covered in class, you can do a Google search; for best results, use the https://rseek.org/ search engine, which limits the results to those relating to the R language.\nYou can also search for answers on Stack Overflow, which is a popular help and discussion website for programmers. You can also post a question there, but make sure to follow community standards and advice on how to ask a good question and how to provide a minimal reproducible example. You will need some experience using the site before being able to ask a good question, but it’s more than certain that any quesiton you have at this stage will have an answer already somewhere.\n\n\nAny further study resources will be listed here."
  },
  {
    "objectID": "materials/index.html",
    "href": "materials/index.html",
    "title": "Materials",
    "section": "",
    "text": "Materials for each week are available from the side menu. The table below outlines the weekly topics.\n\n\n\n\n\n\nWeekly topics\n\n\n\n\n\n\n\n\nWeek 1  Gamblers, God, Guinness and peas\n\n\nA brief history of statistics\n\n\n\n\nWeek 2  Revisiting Flatland\n\n\nA review of general linear models\n\n\n\n\nWeek 3  Dear Prudence, Help! I may be cheating with my X\n\n\nInteractions and the logic of causal inference\n\n\n\n\nWeek 4  The Y question\n\n\nGeneralised linear models\n\n\n\n\nWeek 5  Do we live in a simulation?\n\n\nBasic data simulation for statistical inference and power analysis\n\n\n\n\nWeek 6  Challenging hierarchies\n\n\nMultilevel models\n\n\n\n\nWeek 7  The unobserved\n\n\nLatent variables and structural models\n\n\n\n\nWeek 8  Words, words, mere words…\n\n\nText as data\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assessment/index.html",
    "href": "assessment/index.html",
    "title": "HSS8005 {{< iconify line-md plus >}}",
    "section": "",
    "text": "The final assignment is a 3,500-word long research report."
  }
]